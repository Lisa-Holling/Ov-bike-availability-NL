{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Webscraper for ovfietsbeschikbaar.nl\n",
    "\n",
    "For the course online data collection and management, our team scraped data from all stations in the Netherlands that provide OV-bikes. NS, the railway company of the Netherlands, provide these so called OV-bikes. There are in total 284 stations in the Netherlands where you can rent the bikes. We scraped data of all the 284 stations in the Netherlands for a period of 7 days with an interval of 15 minutes. We collected data about the number of bikes that are available in total and the current availibility of bikes. Moreover, we collected standard information about the type of bicycle storage and the address. We also stored the date and time of when the data was scraped, so we knew about which moment the availibility of bikes was. To give an overview, we scraped:\n",
    "\n",
    "* Date\n",
    "* Time\n",
    "* Name of the station\n",
    "* Total bikes \n",
    "* Current availability\n",
    "* Type of facility\n",
    "* Address\n",
    "\n",
    "As mentioned before, we scraped for a period of 7 days and with intervals of 15 minutes. Of course, it is impossible to scrape 7 days in a row, every 15 minutes manually. We did this with the help of a virtual computer. We copied our code to a virtual computer and ran our Python script every 15 minutes with the help of a cronjob. The type of virtual computer we used was an EC2 instance offered by Amazon Web Services (AWS). AWS also offers a so called s3 bucket; this is a cloud storage service and it allows you to store and retrieve any amount of data from anywhere on the web. We used this tool to automatically store the data every 15 minutes in the cloud, so we were sure the data was stored safely. We can show the roadmap of our project graphically in the graph below, were the project is divided into three parts:\n",
    "\n",
    "<img src=\"https://scrapeovfiets.s3.amazonaws.com/Roadmap.png\" alt=\"Roadmap\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Write scraper code in Python\n",
    "The first step in our process was just simply setting up our web scraper which is just the basic of scraping OV-bikes data from ovfietsbeschikbaar.nl. This can be subdivided into 8 steps, which we will go over one by one:\n",
    "### 1.1 Define the URL of the website and the user agent header\n",
    "We firstly define the base URL from which we want to scrape the data. With the user agent, we let Python know for which browser version to retrieve the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import os, sys\n",
    "import boto3\n",
    "\n",
    "# Define the URL of the website and the user agent header\n",
    "url = 'https://ovfietsbeschikbaar.nl/locaties'\n",
    "header = {'User-agent': 'Mozilla/5.0'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Get the HTML code from the website and process it with BeautifulSoup\n",
    "BeautifulSoup makes it possible to extract information from the source code object. BeautifulSoup provides methods and attributes for navigating and manipulating the HTML document structure, which we will use in the next step to extract data from the soup0 object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HTML code from the website and process it with BeautifulSoup\n",
    "res = requests.get(url, headers=header)\n",
    "res.encoding = res.apparent_encoding\n",
    "source_code_0 = res.text  # make website html code readable as text\n",
    "\n",
    "# Make information \"extractable\" using BeautifulSoup\n",
    "soup0 = BeautifulSoup(source_code_0, 'html.parser')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Store the raw data in a dictionary\n",
    "We store the raw data in a dictionary. This provides a record of the exact HTML source code that was used to extract the data. This can be useful for debugging and troubleshooting purposes, as it allows us (and others that in the future) to see the exact data that was being worked with at the time of extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the raw data in a dictionary\n",
    "raw_data_hoofdlink = {'html': str(soup0)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Find the list of locations on the page and get the links to the OV-fiets stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the list of locations on the page and get the links to the OV-fiets stations\n",
    "locatielijst = soup0.find('a', attrs={'name': 'Locatielijst'})\n",
    "stations = [a['href'] for a in locatielijst.find_next('div').find_all('a', {'class': 'panel-block'})]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Create a list of complete links to the OV-fiets stations\n",
    "We start with an empty list and write a for loop that goes over the list of stations we created in the previous code chunk. For every station it creates a link of the base URL of the website and includes the tag of the station. This way we had a list of seeds to use in the next parts to iterate through and extract data from each sub-page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of links to the OV-fiets stations\n",
    "links = []\n",
    "for station in stations:\n",
    "    combined_link = \"https://ovfietsbeschikbaar.nl\" + station\n",
    "    links.append(combined_link)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Define a function to extract information from each OV-fiets station webpage\n",
    "This function uses as input an url of a webpage. For this webpage, relevant information is scraped. In some cases, an (el)if-else statement needed to be put in place, since sometimes, there was no data available in the place we expected it. We then made sure this error was replaced with relevant feedback and we did not just made our code ignore the errors, since this would cause our code to be very prone to mistakes. \n",
    "We also listed the date and time of extraction with the datetime package. Eventually, we stored all this scraped data in a dictionary. We again made sure to store the raw data for all the individual links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parse_website function to extract information from each OV-fiets station webpage\n",
    "def parse_website(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}  # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "    request = requests.get(url, headers=header)\n",
    "    request.encoding = request.apparent_encoding  # set encoding to UTF-8\n",
    "    source_code = request.text  # make website html code readable as text\n",
    "\n",
    "    # Make information \"extractable\" using BeautifulSoup\n",
    "    soup = BeautifulSoup(source_code, 'html.parser')\n",
    "\n",
    "    # Scrape the relevant information\n",
    "    station = soup.find(class_='title has-text-weight-bold').get_text()\n",
    "    totaal = soup.find(title='Schatting op basis van de laaste drie maanden').get_text(strip=True)\n",
    "    if soup.find(class_=\"grafiek-overlay\").get_text(strip=True) == 'geen actueledata beschikbaar':\n",
    "        beschikbaar = 'Geen actuele data beschikbaar'\n",
    "    elif soup.find(\"div\", {\"class\": \"grafiek-overlay\"}).find(\"span\", {\"class\": \"badge\"})['data-badge'] == '!':\n",
    "        beschikbaar = 'Vertraging in de data'\n",
    "    else:\n",
    "        beschikbaar = soup.find('td', string='Nu beschikbaar').find_next_sibling('td').get_text(strip=True)\n",
    "    soort = soup.find('td', string='Type stalling').find_next_sibling('td').get_text(strip=True)\n",
    "    adres = soup.find(class_='table is-narrow is-fullwidth').find('td').get_text()\n",
    "    if adres.startswith('Adres:'):\n",
    "        adres = adres.replace('Adres:', '')\n",
    "    else:\n",
    "        adres = 'Onbekend'\n",
    "\n",
    "    # extract current date and time\n",
    "    current_datetime = datetime.datetime.now(pytz.timezone('Europe/Amsterdam'))\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # store the information in a dictionary\n",
    "    data = {'date': formatted_datetime,\n",
    "            'station': station,\n",
    "    \t\t'totaal': totaal,\n",
    "    \t\t'beschikbaar': beschikbaar,\n",
    "        'type stalling': soort,\n",
    "        'adres': adres}\n",
    "\n",
    "    # store the raw data in a dictionary\n",
    "    raw_data = {'date': formatted_datetime, 'html': str(soup)}\n",
    "\n",
    "    return (data, raw_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Store the raw HTML content in a JSON file\n",
    "We can store the raw data in a file called raw_data.json and the actual scraped data in a file called ov_data.json. This way, we have two seperate files, one with the source code of every single page scraped and one with the structured scraped data (see 1.8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the raw data of the html structure in a json file\n",
    "f = open('raw_data.json', 'a', encoding='utf-8')\n",
    "f.write(json.dumps(raw_data_hoofdlink))\n",
    "f.write('\\n')  # new line to separate objects\n",
    "f.close()\n",
    "\n",
    "f = open('raw_data.json', 'a', encoding='utf-8')\n",
    "for link in links:\n",
    "    raw_data = parse_website(link)[1]\n",
    "    f.write(json.dumps(raw_data))\n",
    "    f.write('\\n')  # new line to separate objects\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Store the scraped data  in a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the scraped data in a json file\n",
    "f = open('ov_data.json', 'a', encoding='utf-8')\n",
    "for link in links:\n",
    "    data = parse_website(link)[0]\n",
    "    f.write(json.dumps(data))\n",
    "    f.write('\\n')  # new line to separate objects\n",
    "f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set up EC2 instance\n",
    "After creating this python scraping code, we decided to set up an EC2 instance to run scheduled tasks. One of the main purposes is to automate running our python code that need to be executed on a regular basis. By setting up a scheduled task on our EC2 instance, the task can be executed automatically at specific intervals, without requiring manual intervention. Moreover, we could leave our python code running every 15 minutes, even if we shut down the laptop."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Launch an EC2 instance via AWS\n",
    "We launched an EC2 instance via Amazon Web Services (AWS) by following the steps from the [Running Computations Remotely article](https://tilburgsciencehub.com/tutorials/scale-up/running-computations-remotely/cloud-computing/) on Tilburg Science hub. This way, we got an EC2 instance as shown in the picture below:\n",
    "\n",
    "<img src=\"https://scrapeovfiets.s3.amazonaws.com/EC2.png\" alt=\"EC2 instance\" width = \"900\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Connect to EC2 instance\n",
    "The next step was to connect to the EC2 instance via our terminal. this had to be done by specifying the HOST and the KEY variable by running the following commands in our terminal:\n",
    "```dash\n",
    "export HOST=<OUR-PUBLIC-IPV4-DNS>\n",
    "export KEY=<PATH_TO_OUR_KEY_PAIR_FILE>\n",
    "```\n",
    "You than use the `ssh` command to actualy connect to the instance:\n",
    "```dash\n",
    "ssh -i /path/to/key.pem ec2-user@<EC2-instance-DNS>\n",
    "```\n",
    "We then got the following output in our terminal, which meant we were connected to our EC2 instance:\n",
    "\n",
    "<img src=\"https://scrapeovfiets.s3.amazonaws.com/Connecting_to_EC2.png\" alt=\"Connecting to EC2 instance\" width = \"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Move python script from own computer to EC2 instance\n",
    "The next step was to move our python script to our EC2 instance so we can, in the next step, run it via our cronjob. First, we needed to make sure we were in the directory where our python script was located via the terminal (with cd commands). We then ran one line of code to copy files to the EC2 instance:\n",
    "```dash\n",
    "scp -i $KEY -r $(pwd) ec2-user@$HOST:/home/ec2-user\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Set up a cronjob to run the code every 15 minutes\n",
    "The code now needed to be executed every 15 minutes automatically. This was done by setting up a cronjob on our EC2 instance. In small steps, the following was typed in our terminal while connected with the EC2 instance:\n",
    "1. ` crontab -e `: this opened the vim editor\n",
    "2. `I`: this allowed us to type a cronjob in the vim editor\n",
    "3. `*/15 * * * * /usr/bin/python3 /home/ec2-user/ScrapeFiets/ScrapeFiets.py `: the cron syntax '`*/15 * * * *`' means that the code runs all the time with an interval of 15 minutes. `/usr/bin/python3` is the path to the python installation on our EC2 instance. To know this path to our python installation, we runned the code below once in python. `/home/ec2-user/ScrapeFiets/ScrapeFiets.py ` is the path to our python script.\n",
    "4. ` :wq `: this way, we closed the vim editor and saved the cronjob. \n",
    "\n",
    "Now we could verify our crontab was set up by running `crontab -l` and seeing our crontab line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the path to the python installation on the EC2 instance\n",
    "path = os.path.dirname(sys.executable)\n",
    "print(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linking to a S3 bucket\n",
    "The last step was to link our EC2 instance to an s3 bucket. An s3 bucket is a cloud-based object storage service also offered by Amazon Web Services (AWS).\n",
    "We used S3 buckets for backup and recovery, since we automatically uploaded the data to the s3 bucket every 15 minutes. This way, it can be accessed when needed and even when, in extreme cases, the EC2 instance crashes or gets hacked, we still have the lastest retrieved data in the cloud in our s3 bucket. An s3 bucket can be set up in a few steps:\n",
    "\n",
    "### 3.1 Creating an s3 bucket\n",
    "First of all, we created an s3 bucket on our same AWS account as on which we created our EC2 instance. This was not that difficult and could be set up in minutes (see picture). Our s3 bucket was called 'scrapeovfiets'. The difficult part was creating the IAM role and linking the s3 bucket to the EC2 instance with the IAM role (see 3.2 and 3.3). \n",
    "\n",
    "<img src=\"https://scrapeovfiets.s3.amazonaws.com/bucket.png\" alt=\"s3 bucket\" width = \"900\">\n",
    "\n",
    "### 3.2 Creating an IAM role with s3:PutObject permissions\n",
    "For the EC2 instance to be allowed to put objects in the s3 bucket, it needed to be assigned an IAM role. An IAM role can be compared to a permission that a mother grants to her child to play with certain toys in the house. The EC2 instance is the child, and the S3 bucket is the toy box. We thus needed to set up an IAM role in two steps:\n",
    "1. Create a policy\n",
    "\n",
    "    We first need to create a policy which is a permission to put objects in the s3 bucket (policies and permissions are used both, but have the same meaning). We did this by tying the following json formatted code in the json editor for creating a policy:\n",
    "\n",
    "    ```json\n",
    "    {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"S3PutObject\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:PutObject\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"arn:aws:s3:::scrapeovfiets/*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "    We then could give our policy a name and save it. \n",
    "\n",
    "2. Create IAM role and assign policy\n",
    "\n",
    "    Then we create an actual IAM role. In this step we can assign the policy created in step 3.2 to our IAM role. We then give our IAM role a name and save it. \n",
    "\n",
    "\n",
    "### 3.3 Assign IAM role to the EC2 instance\n",
    "So far, we have created an s3 bucket and we created an IAM role with s3:PutObject permissions. We now want to assign this IAM role to the EC2 instance, so the EC2 instance is allowed to put objects in the s3 bucket (we want the mother to give permissioins to the child to play with the toys). We do this by just connecting the IAM role to the EC2 instance.\n",
    "\n",
    "<img src=\"https://scrapeovfiets.s3.amazonaws.com/AssignIAMrole.png\" alt=\"Assign IAM role\" width = \"900\">\n",
    "\n",
    "After we have connected the IAM role with the EC2 instance, we can tes in our terminal if it worked. For this, we have to connect with the EC2 instance in our terminal and type the following command in our terminal:\n",
    "```dash\n",
    "aws iam list-users\n",
    "```\n",
    "If everything worked, we should get the output:\n",
    "\n",
    "<img src=\"https://scrapeovfiets.s3.amazonaws.com/IAMroleterminal.png\" alt=\"Assign IAM role\" width = \"350\">\n",
    "\n",
    "### 3.4 Include code in Python script that puts the json files in the s3 bucket\n",
    "The last step was to include code in our Python script that puts the json files created in the script in our s3 bucket. This can be done with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure s3 bucket can be used (define parameters)\n",
    "bucket_name = 'scrapeovfiets'\n",
    "destination_file_key_data = 'ov_data.json'\n",
    "destination_file_key_html = 'raw_data.json'\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the file to S3\n",
    "s3.put_object(Body=open('ov_data.json', 'rb'), Bucket=bucket_name, Key=destination_file_key_data)\n",
    "s3.put_object(Body=open('raw_data.json', 'rb'), Bucket=bucket_name, Key=destination_file_key_html)\n",
    "\n",
    "time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up\n",
    "We now have set up an virtual machine that runs our python script every 15 minutes and stores the json files with data created in this pyhton code in the cloud! \n",
    "\n",
    "The code in one block is pasted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "import pytz\n",
    "import time\n",
    "import os, sys\n",
    "import boto3\n",
    "\n",
    "# finding the path to the python installation on the EC2 instance\n",
    "path = os.path.dirname(sys.executable)\n",
    "print(path)\n",
    "\n",
    "# make sure s3 bucket can be used\n",
    "bucket_name = 'scrapeovfiets'\n",
    "destination_file_key_data = 'ov_data.json'\n",
    "destination_file_key_html = 'raw_data.json'\n",
    "\n",
    "# Define the URL of the website and the user agent header\n",
    "url = 'https://ovfietsbeschikbaar.nl/locaties'\n",
    "header = {'User-agent': 'Mozilla/5.0'}\n",
    "# Get the HTML code from the website and process it with BeautifulSoup\n",
    "res = requests.get(url, headers=header)\n",
    "res.encoding = res.apparent_encoding\n",
    "source_code_0 = res.text  # make website html code readable as text\n",
    "\n",
    "# Make information \"extractable\" using BeautifulSoup\n",
    "soup0 = BeautifulSoup(source_code_0, 'html.parser')\n",
    "\n",
    "# store the raw data in a dictionary\n",
    "raw_data_hoofdlink = {'html': str(soup0)}\n",
    "\n",
    "# Find the list of locations on the page and get the links to the OV-fiets stations\n",
    "locatielijst = soup0.find('a', attrs={'name': 'Locatielijst'})\n",
    "stations = [a['href'] for a in locatielijst.find_next('div').find_all('a', {'class': 'panel-block'})]\n",
    "\n",
    "# Create a list of links to the OV-fiets stations\n",
    "links = []\n",
    "for station in stations:\n",
    "    combined_link = \"https://ovfietsbeschikbaar.nl\" + station\n",
    "    links.append(combined_link)\n",
    "\n",
    "# Define the parse_website function to extract information from each OV-fiets station webpage\n",
    "def parse_website(url):\n",
    "    header = {'User-agent': 'Mozilla/5.0'}  # with the user agent, we let Python know for which browser version to retrieve the website\n",
    "    request = requests.get(url, headers=header)\n",
    "    request.encoding = request.apparent_encoding  # set encoding to UTF-8\n",
    "    source_code = request.text  # make website html code readable as text\n",
    "\n",
    "    # Make information \"extractable\" using BeautifulSoup\n",
    "    soup = BeautifulSoup(source_code, 'html.parser')\n",
    "\n",
    "    # Scrape the relevant information\n",
    "    station = soup.find(class_='title has-text-weight-bold').get_text()\n",
    "    totaal = soup.find(title='Schatting op basis van de laaste drie maanden').get_text(strip=True)\n",
    "    if soup.find(class_=\"grafiek-overlay\").get_text(strip=True) == 'geen actueledata beschikbaar':\n",
    "        beschikbaar = 'Geen actuele data beschikbaar'\n",
    "    elif soup.find(\"div\", {\"class\": \"grafiek-overlay\"}).find(\"span\", {\"class\": \"badge\"})['data-badge'] == '!':\n",
    "        beschikbaar = 'Vertraging in de data'\n",
    "    else:\n",
    "        beschikbaar = soup.find('td', string='Nu beschikbaar').find_next_sibling('td').get_text(strip=True)\n",
    "    soort = soup.find('td', string='Type stalling').find_next_sibling('td').get_text(strip=True)\n",
    "    adres = soup.find(class_='table is-narrow is-fullwidth').find('td').get_text()\n",
    "    if adres.startswith('Adres:'):\n",
    "        adres = adres.replace('Adres:', '')\n",
    "    else:\n",
    "        adres = 'Onbekend'\n",
    "\n",
    "    # extract current date and time\n",
    "    current_datetime = datetime.datetime.now(pytz.timezone('Europe/Amsterdam'))\n",
    "    formatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    # store the information in a dictionary\n",
    "    data = {'date': formatted_datetime,\n",
    "            'station': station,\n",
    "    \t\t'totaal': totaal,\n",
    "    \t\t'beschikbaar': beschikbaar,\n",
    "        'type stalling': soort,\n",
    "        'adres': adres}\n",
    "\n",
    "    # store the raw data in a dictionary\n",
    "    raw_data = {'date': formatted_datetime, 'html': str(soup)}\n",
    "\n",
    "    return (data, raw_data)\n",
    "\n",
    "# store the raw data of the html structure in a json file\n",
    "f = open('raw_data.json', 'a', encoding='utf-8')\n",
    "f.write(json.dumps(raw_data_hoofdlink))\n",
    "f.write('\\n')  # new line to separate objects\n",
    "f.close()\n",
    "\n",
    "f = open('raw_data.json', 'a', encoding='utf-8')\n",
    "for link in links:\n",
    "    raw_data = parse_website(link)[1]\n",
    "    f.write(json.dumps(raw_data))\n",
    "    f.write('\\n')  # new line to separate objects\n",
    "f.close()\n",
    "\n",
    "# store the scraped data in a json file\n",
    "f = open('ov_data.json', 'a', encoding='utf-8')\n",
    "for link in links:\n",
    "    data = parse_website(link)[0]\n",
    "    f.write(json.dumps(data))\n",
    "    f.write('\\n')  # new line to separate objects\n",
    "f.close()\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload the file to S3\n",
    "s3.put_object(Body=open('ov_data.json', 'rb'), Bucket=bucket_name, Key=destination_file_key_data)\n",
    "s3.put_object(Body=open('raw_data.json', 'rb'), Bucket=bucket_name, Key=destination_file_key_html)\n",
    "\n",
    "time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
